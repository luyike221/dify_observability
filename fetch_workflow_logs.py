#!/usr/bin/env python3
"""
Dify å·¥ä½œæµæ—¥å¿—è·å–è„šæœ¬

ç”¨äºä»å¤–éƒ¨ç³»ç»Ÿè·å– Dify å·¥ä½œæµåº”ç”¨çš„æ‰§è¡Œæ—¥å¿—æ•°æ®ï¼Œé€‚ç”¨äºè¿ç»´ç›‘æ§åœºæ™¯ã€‚

ä½¿ç”¨æ–¹æ³•:
    python fetch_workflow_logs.py --api-token <token> --base-url <url> [é€‰é¡¹]

ç¤ºä¾‹:
    # åŸºæœ¬ç”¨æ³•
    python fetch_workflow_logs.py --api-token app-R7kB8EZhzg4R8o2Li6FA4Dgb --base-url http://localhost

    # è·å–å¤±è´¥çš„æ—¥å¿—
    python fetch_workflow_logs.py --api-token app-R7kB8EZhzg4R8o2Li6FA4Dgb --base-url http://localhost --status failed

    # æœç´¢å…³é”®è¯å¹¶æŒ‡å®šæ—¶é—´èŒƒå›´
    python fetch_workflow_logs.py --api-token app-R7kB8EZhzg4R8o2Li6FA4Dgb --base-url http://localhost \\
        --keyword "error" --after "2024-01-01T00:00:00Z" --before "2024-01-31T23:59:59Z"

    # å¯¼å‡ºä¸º JSON æ–‡ä»¶
    python fetch_workflow_logs.py --api-token app-R7kB8EZhzg4R8o2Li6FA4Dgb --base-url http://localhost --output logs.json

    # å¯¼å‡ºä¸º Markdown æ–‡ä»¶ï¼ˆåŒ…å«æ•´ä½“æ‘˜è¦å’Œæ¯æ¡æ—¥å¿—çš„è¯¦ç»†ä¿¡æ¯ï¼‰
    python fetch_workflow_logs.py --api-token app-R7kB8EZhzg4R8o2Li6FA4Dgb --base-url http://localhost \
        --with-details --output-md logs_report.md

    # è·å–è¯¦ç»†ä¿¡æ¯ï¼ˆå·¥ä½œæµè¿è¡Œè¯¦æƒ…ï¼‰
    python fetch_workflow_logs.py --api-token app-R7kB8EZhzg4R8o2Li6FA4Dgb --base-url http://localhost --with-details

    # è·å–å®Œæ•´ä¿¡æ¯ï¼ˆåŒ…æ‹¬èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…ï¼Œä½¿ç”¨è´¦å·å¯†ç è‡ªåŠ¨è·å– Console Tokenï¼‰
    python fetch_workflow_logs.py --api-token app-R7kB8EZhzg4R8o2Li6FA4Dgb --base-url http://localhost \
        --with-details --with-node-executions \
        --console-email 248618931@qq.com --console-password zj0309.. \
        --app-id 5ac73990-ee17-4aba-993c-a473e2fa2a90

    # æˆ–è€…ä½¿ç”¨å·²æœ‰çš„ Console Token
    python fetch_workflow_logs.py --api-token app-R7kB8EZhzg4R8o2Li6FA4Dgb --base-url http://localhost \
        --with-details --with-node-executions \
        --console-token <your_token> \
        --app-id 5ac73990-ee17-4aba-993c-a473e2fa2a90

    # ç”Ÿæˆ CSV æŠ¥å‘Šï¼ˆé—®ç­”ç±»åº”ç”¨æ•°ç›¸å…³ç»Ÿè®¡ï¼‰
    python 01-difyè¿ç»´ç›‘æ§/fetch_workflow_logs.py --api-token app-R7kB8EZhzg4R8o2Li6FA4Dgb --base-url http://localhost \
        --with-details --with-node-executions \
        --console-email 248618931@qq.com --console-password zj0309.. \
        --app-id 5ac73990-ee17-4aba-993c-a473e2fa2a90 \
        --output-csv-dir .
å…³äº Console Token:
    Console Token æ˜¯ç”¨æˆ·ç™»å½• Dify æ§åˆ¶å°åè·å¾—çš„ JWT tokenï¼Œç”¨äºè®¿é—® Console APIã€‚
    
    è·å–æ–¹å¼ï¼ˆæ¨èä½¿ç”¨æ–¹å¼ 1ï¼Œè‡ªåŠ¨è·å–ï¼‰ï¼š
    1. è‡ªåŠ¨è·å–ï¼ˆæ¨èï¼‰ï¼š
       ä½¿ç”¨ --console-email å’Œ --console-password å‚æ•°ï¼Œè„šæœ¬ä¼šè‡ªåŠ¨ç™»å½•å¹¶è·å– token
       å¦‚æœ token å¤±æ•ˆï¼Œè„šæœ¬ä¼šè‡ªåŠ¨é‡æ–°ç™»å½•è·å–æ–°çš„ token
       
       ç¤ºä¾‹ï¼š
       --console-email your-email@example.com --console-password your-password
    
    2. æ‰‹åŠ¨æä¾› Tokenï¼š
       ä½¿ç”¨ --console-token å‚æ•°ç›´æ¥æä¾›å·²è·å–çš„ token
       
       è·å–æ–¹æ³•ï¼š
       - é€šè¿‡ç™»å½•æ¥å£ï¼š
         curl -X POST "http://localhost/console/api/login" \\
           -H "Content-Type: application/json" \\
           -d '{"email": "your-email@example.com", "password": "your-password"}'
       
       - ä»æµè§ˆå™¨å¼€å‘è€…å·¥å…·ï¼š
         ç™»å½• Dify æ§åˆ¶å° -> F12 -> Network -> æŸ¥çœ‹ Console API è¯·æ±‚çš„ Authorization header
    
    æ³¨æ„ï¼š
    - å¦‚æœåªéœ€è¦å·¥ä½œæµè¿è¡Œè¯¦æƒ…ï¼Œä¸éœ€è¦ Console Tokenï¼ˆä½¿ç”¨ --with-details å³å¯ï¼‰
    - åªæœ‰éœ€è¦èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…æ—¶æ‰éœ€è¦ Console Tokenï¼ˆä½¿ç”¨ --with-node-executionsï¼‰
    - ä½¿ç”¨è´¦å·å¯†ç æ–¹å¼æ—¶ï¼Œtoken å¤±æ•ˆä¼šè‡ªåŠ¨é‡æ–°è·å–ï¼Œæ— éœ€æ‰‹åŠ¨æ“ä½œ
"""

import argparse
import csv
import json
import sys
from collections import defaultdict
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

try:
    import requests
except ImportError:
    print("é”™è¯¯: éœ€è¦å®‰è£… requests åº“")
    print("è¯·è¿è¡Œ: pip install requests")
    sys.exit(1)


class WorkflowLogFetcher:
    """å·¥ä½œæµæ—¥å¿—è·å–å™¨"""

    def __init__(
        self,
        base_url: str,
        api_token: str,
        console_token: Optional[str] = None,
        console_email: Optional[str] = None,
        console_password: Optional[str] = None,
    ):
        """
        åˆå§‹åŒ–æ—¥å¿—è·å–å™¨

        Args:
            base_url: Dify API åŸºç¡€ URL (ä¾‹å¦‚: https://api.dify.ai)
            api_token: åº”ç”¨ API Token (æ ¼å¼: app-xxx)
            console_token: Console API Token (å¯é€‰ï¼Œç”¨äºè·å–èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…)
            console_email: Console ç™»å½•é‚®ç®± (å¯é€‰ï¼Œç”¨äºè‡ªåŠ¨è·å– token)
            console_password: Console ç™»å½•å¯†ç  (å¯é€‰ï¼Œç”¨äºè‡ªåŠ¨è·å– token)
        """
        self.base_url = base_url.rstrip("/")
        self.api_token = api_token
        self.console_token = console_token
        self.console_email = console_email
        self.console_password = console_password
        
        self.session = requests.Session()
        self.session.headers.update({
            "Authorization": f"Bearer {api_token}",
            "Content-Type": "application/json",
        })
        
        # Console API session (ç”¨äºè·å–èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…)
        self.console_session = None
        if console_token:
            self._init_console_session(console_token)
        elif console_email and console_password:
            # è‡ªåŠ¨è·å– console token
            self._auto_login_console()

    def _init_console_session(self, token: str):
        """åˆå§‹åŒ– Console API session"""
        self.console_token = token
        self.console_session = requests.Session()
        self.console_session.headers.update({
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json",
        })

    def _auto_login_console(self) -> bool:
        """
        è‡ªåŠ¨ç™»å½• Console API è·å– token

        Returns:
            æ˜¯å¦æˆåŠŸè·å– token
        """
        if not self.console_email or not self.console_password:
            return False

        try:
            url = f"{self.base_url}/console/api/login"
            response = requests.post(
                url,
                json={
                    "email": self.console_email,
                    "password": self.console_password,
                },
                timeout=30,
            )
            response.raise_for_status()
            result = response.json()
            
            if result.get("result") == "success":
                data = result.get("data", {})
                access_token = data.get("access_token")
                if access_token:
                    self._init_console_session(access_token)
                    print(f"âœ… å·²è‡ªåŠ¨è·å– Console Token (ç”¨æˆ·: {self.console_email})")
                    return True
                else:
                    print(f"âŒ ç™»å½•æˆåŠŸä½†æœªè·å–åˆ° access_token")
                    return False
            else:
                error_msg = result.get("data", "æœªçŸ¥é”™è¯¯")
                print(f"âŒ ç™»å½•å¤±è´¥: {error_msg}")
                return False
        except requests.exceptions.RequestException as e:
            print(f"âŒ è‡ªåŠ¨ç™»å½•å¤±è´¥: {str(e)}")
            return False

    def _ensure_console_token(self) -> bool:
        """
        ç¡®ä¿ Console Token æœ‰æ•ˆï¼Œå¦‚æœå¤±æ•ˆåˆ™è‡ªåŠ¨é‡æ–°è·å–

        Returns:
            Console session æ˜¯å¦å¯ç”¨
        """
        if not self.console_session:
            # å¦‚æœæ²¡æœ‰ sessionï¼Œå°è¯•è‡ªåŠ¨ç™»å½•
            if self.console_email and self.console_password:
                return self._auto_login_console()
            return False
        
        # å¦‚æœå·²æœ‰ sessionï¼Œç›´æ¥è¿”å› True
        # token å¤±æ•ˆä¼šåœ¨å®é™…è¯·æ±‚æ—¶æ£€æµ‹å¹¶é‡æ–°è·å–
        return True

    def _handle_console_auth_error(self):
        """å¤„ç† Console API è®¤è¯é”™è¯¯ï¼Œå°è¯•é‡æ–°ç™»å½•"""
        if self.console_email and self.console_password:
            print("âš ï¸  Console Token å¯èƒ½å·²å¤±æ•ˆï¼Œå°è¯•é‡æ–°ç™»å½•...")
            if self._auto_login_console():
                return True
        return False

    def fetch_logs(
        self,
        keyword: Optional[str] = None,
        status: Optional[str] = None,
        created_at_before: Optional[str] = None,
        created_at_after: Optional[str] = None,
        created_by_end_user_session_id: Optional[str] = None,
        created_by_account: Optional[str] = None,
        page: int = 1,
        limit: int = 20,
    ) -> Dict[str, Any]:
        """
        è·å–å·¥ä½œæµæ—¥å¿—

        Args:
            keyword: æœç´¢å…³é”®è¯
            status: æ‰§è¡ŒçŠ¶æ€ (succeeded/failed/stopped/partial-succeeded)
            created_at_before: åˆ›å»ºæ—¶é—´ä¸Šé™ (ISO 8601 æ ¼å¼)
            created_at_after: åˆ›å»ºæ—¶é—´ä¸‹é™ (ISO 8601 æ ¼å¼)
            created_by_end_user_session_id: ç»ˆç«¯ç”¨æˆ·ä¼šè¯ID
            created_by_account: è´¦æˆ·é‚®ç®±
            page: é¡µç 
            limit: æ¯é¡µæ•°é‡

        Returns:
            åŒ…å«æ—¥å¿—æ•°æ®çš„å­—å…¸
        """
        url = f"{self.base_url}/v1/workflows/logs"
        params = {
            "page": page,
            "limit": limit,
        }

        if keyword:
            params["keyword"] = keyword
        if status:
            params["status"] = status
        if created_at_before:
            params["created_at__before"] = created_at_before
        if created_at_after:
            params["created_at__after"] = created_at_after
        if created_by_end_user_session_id:
            params["created_by_end_user_session_id"] = created_by_end_user_session_id
        if created_by_account:
            params["created_by_account"] = created_by_account

        try:
            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            if hasattr(e.response, "text"):
                error_msg = f"è¯·æ±‚å¤±è´¥: {e.response.status_code} - {e.response.text}"
            else:
                error_msg = f"è¯·æ±‚å¤±è´¥: {str(e)}"
            raise Exception(error_msg) from e

    def fetch_all_logs(
        self,
        keyword: Optional[str] = None,
        status: Optional[str] = None,
        created_at_before: Optional[str] = None,
        created_at_after: Optional[str] = None,
        created_by_end_user_session_id: Optional[str] = None,
        created_by_account: Optional[str] = None,
        limit: int = 20,
        max_pages: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰æ—¥å¿—ï¼ˆè‡ªåŠ¨ç¿»é¡µï¼‰

        Args:
            keyword: æœç´¢å…³é”®è¯
            status: æ‰§è¡ŒçŠ¶æ€
            created_at_before: åˆ›å»ºæ—¶é—´ä¸Šé™
            created_at_after: åˆ›å»ºæ—¶é—´ä¸‹é™
            created_by_end_user_session_id: ç»ˆç«¯ç”¨æˆ·ä¼šè¯ID
            created_by_account: è´¦æˆ·é‚®ç®±
            limit: æ¯é¡µæ•°é‡
            max_pages: æœ€å¤§é¡µæ•°é™åˆ¶ï¼ˆNone è¡¨ç¤ºæ— é™åˆ¶ï¼‰

        Returns:
            æ‰€æœ‰æ—¥å¿—è®°å½•çš„åˆ—è¡¨
        """
        all_logs = []
        page = 1

        while True:
            if max_pages and page > max_pages:
                break

            result = self.fetch_logs(
                keyword=keyword,
                status=status,
                created_at_before=created_at_before,
                created_at_after=created_at_after,
                created_by_end_user_session_id=created_by_end_user_session_id,
                created_by_account=created_by_account,
                page=page,
                limit=limit,
            )

            logs = result.get("data", [])
            if not logs:
                break

            all_logs.extend(logs)

            if not result.get("has_more", False):
                break

            page += 1

        return all_logs

    def fetch_workflow_run_detail(self, workflow_run_id: str) -> Optional[Dict[str, Any]]:
        """
        è·å–å·¥ä½œæµè¿è¡Œè¯¦æƒ…

        Args:
            workflow_run_id: å·¥ä½œæµè¿è¡ŒID

        Returns:
            å·¥ä½œæµè¿è¡Œè¯¦æƒ…å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
        """
        url = f"{self.base_url}/v1/workflows/run/{workflow_run_id}"
        
        try:
            response = self.session.get(url, timeout=30)
            if response.status_code == 404:
                return None
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            if hasattr(e, "response") and e.response is not None:
                if e.response.status_code == 404:
                    return None
                error_msg = f"è¯·æ±‚å¤±è´¥: {e.response.status_code} - {e.response.text}"
            else:
                error_msg = f"è¯·æ±‚å¤±è´¥: {str(e)}"
            raise Exception(error_msg) from e

    def fetch_node_executions(self, app_id: str, workflow_run_id: str) -> List[Dict[str, Any]]:
        """
        è·å–å·¥ä½œæµè¿è¡Œçš„èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…

        Args:
            app_id: åº”ç”¨ID
            workflow_run_id: å·¥ä½œæµè¿è¡ŒID

        Returns:
            èŠ‚ç‚¹æ‰§è¡Œåˆ—è¡¨ï¼Œå¦‚æœæ— æ³•è·å–åˆ™è¿”å›ç©ºåˆ—è¡¨
        """
        if not self._ensure_console_token():
            return []

        url = f"{self.base_url}/console/api/apps/{app_id}/workflow-runs/{workflow_run_id}/node-executions"
        
        try:
            response = self.console_session.get(url, timeout=30)
            if response.status_code == 404:
                return []
            elif response.status_code == 401:
                # Token å¤±æ•ˆï¼Œå°è¯•é‡æ–°ç™»å½•
                if self._handle_console_auth_error():
                    # é‡æ–°è¯·æ±‚
                    response = self.console_session.get(url, timeout=30)
                    if response.status_code == 401:
                        return []
                    response.raise_for_status()
                else:
                    return []
            response.raise_for_status()
            result = response.json()
            return result.get("data", [])
        except requests.exceptions.RequestException:
            # å¦‚æœè·å–å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼ˆä¸æŠ›å‡ºå¼‚å¸¸ï¼‰
            return []

    def enrich_log_with_details(self, log: Dict[str, Any], default_app_id: Optional[str] = None, include_node_executions: bool = False) -> Dict[str, Any]:
        """
        ä¸ºæ—¥å¿—æ·»åŠ è¯¦ç»†ä¿¡æ¯

        Args:
            log: æ—¥å¿—è®°å½•
            default_app_id: é»˜è®¤åº”ç”¨IDï¼ˆå¦‚æœæ—¥å¿—ä¸­æ²¡æœ‰app_idåˆ™ä½¿ç”¨æ­¤å€¼ï¼‰
            include_node_executions: æ˜¯å¦åŒ…å«èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…

        Returns:
            å¢å¼ºåçš„æ—¥å¿—è®°å½•
        """
        workflow_run = log.get("workflow_run", {})
        workflow_run_id = workflow_run.get("id")
        
        if not workflow_run_id:
            return log

        # è·å–å·¥ä½œæµè¿è¡Œè¯¦æƒ…
        try:
            run_detail = self.fetch_workflow_run_detail(workflow_run_id)
            if run_detail:
                log["workflow_run_detail"] = run_detail
        except Exception as e:
            # å¦‚æœè·å–å¤±è´¥ï¼Œè®°å½•ä½†ä¸ä¸­æ–­æµç¨‹
            log["workflow_run_detail_error"] = str(e)

        # è·å–èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…ï¼ˆå¦‚æœéœ€è¦ä¸”æ”¯æŒï¼‰
        if include_node_executions:
            # å°è¯•ä»å¤šä¸ªåœ°æ–¹è·å– app_id
            app_id = (
                log.get("app_id") or  # ä»æ—¥å¿—ä¸­è·å–
                default_app_id or      # ä½¿ç”¨é»˜è®¤å€¼
                (run_detail.get("app_id") if run_detail else None)  # ä»å·¥ä½œæµè¿è¡Œè¯¦æƒ…ä¸­è·å–
            )
            
            if app_id:
                try:
                    node_executions = self.fetch_node_executions(app_id, workflow_run_id)
                    log["node_executions"] = node_executions
                except Exception as e:
                    # å¦‚æœè·å–å¤±è´¥ï¼Œè®°å½•ä½†ä¸ä¸­æ–­æµç¨‹
                    log["node_executions_error"] = str(e)
            else:
                log["node_executions_error"] = "æ— æ³•ç¡®å®š app_id"

        return log


def format_timestamp(timestamp: Optional[float]) -> str:
    """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
    if timestamp is None:
        return "N/A"
    try:
        return datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S")
    except (ValueError, TypeError):
        return str(timestamp)


def format_json_for_markdown(data: Any) -> str:
    """
    æ ¼å¼åŒ– JSON æ•°æ®ä¸º Markdown å‹å¥½çš„æ ¼å¼ï¼Œå¤„ç† Unicode ç¼–ç 
    
    Args:
        data: è¦æ ¼å¼åŒ–çš„æ•°æ®ï¼ˆå¯èƒ½æ˜¯ dictã€list æˆ– JSON å­—ç¬¦ä¸²ï¼‰
    
    Returns:
        æ ¼å¼åŒ–åçš„ JSON å­—ç¬¦ä¸²
    """
    def decode_json_strings(obj: Any) -> Any:
        """é€’å½’è§£ç åµŒå¥—çš„ JSON å­—ç¬¦ä¸²"""
        if isinstance(obj, str):
            # å°è¯•è§£æä¸º JSON
            try:
                parsed = json.loads(obj)
                # å¦‚æœè§£ææˆåŠŸï¼Œé€’å½’å¤„ç†è§£æåçš„å¯¹è±¡
                return decode_json_strings(parsed)
            except (json.JSONDecodeError, TypeError):
                # å¦‚æœä¸æ˜¯æœ‰æ•ˆçš„ JSONï¼Œç›´æ¥è¿”å›å­—ç¬¦ä¸²
                return obj
        elif isinstance(obj, dict):
            # é€’å½’å¤„ç†å­—å…¸çš„æ¯ä¸ªå€¼
            return {k: decode_json_strings(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            # é€’å½’å¤„ç†åˆ—è¡¨çš„æ¯ä¸ªå…ƒç´ 
            return [decode_json_strings(item) for item in obj]
        else:
            # å…¶ä»–ç±»å‹ç›´æ¥è¿”å›
            return obj
    
    # å…ˆè§£ç æ‰€æœ‰åµŒå¥—çš„ JSON å­—ç¬¦ä¸²
    decoded_data = decode_json_strings(data)
    
    # æ ¼å¼åŒ–è¾“å‡ºï¼Œensure_ascii=False ç¡®ä¿ Unicode å­—ç¬¦ç›´æ¥æ˜¾ç¤ºä¸ºæ±‰å­—
    return json.dumps(decoded_data, ensure_ascii=False, indent=2)


def print_logs_table(logs: List[Dict[str, Any]], show_details: bool = False):
    """ä»¥è¡¨æ ¼å½¢å¼æ‰“å°æ—¥å¿—"""
    if not logs:
        print("æ²¡æœ‰æ‰¾åˆ°æ—¥å¿—è®°å½•")
        return

    print(f"\n{'='*120}")
    print(f"{'ID':<38} {'çŠ¶æ€':<12} {'åˆ›å»ºæ—¶é—´':<20} {'è€—æ—¶(ç§’)':<10} {'æ¥æº':<15} {'åˆ›å»ºè€…':<20}")
    print(f"{'-'*120}")

    for log in logs:
        log_id = log.get("id", "N/A")[:36]
        workflow_run = log.get("workflow_run", {})
        status = workflow_run.get("status", "N/A")
        created_at = format_timestamp(log.get("created_at"))
        elapsed_time = workflow_run.get("elapsed_time", 0)
        created_from = log.get("created_from", "N/A")
        
        # è·å–åˆ›å»ºè€…ä¿¡æ¯
        created_by_account = log.get("created_by_account", {})
        created_by_end_user = log.get("created_by_end_user", {})
        if created_by_account:
            creator = created_by_account.get("email", "N/A")
        elif created_by_end_user:
            creator = created_by_end_user.get("session_id", "N/A")
        else:
            creator = "N/A"

        print(f"{log_id:<38} {status:<12} {created_at:<20} {elapsed_time:<10.2f} {created_from:<15} {creator:<20}")
        
        # å¦‚æœæœ‰è¯¦ç»†ä¿¡æ¯ï¼Œæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        if show_details:
            print(f"\n  ğŸ“‹ æ—¥å¿—è¯¦æƒ… (ID: {log_id}):")
            
            # å·¥ä½œæµè¿è¡Œè¯¦æƒ…
            run_detail = log.get("workflow_run_detail")
            if run_detail:
                print(f"    âœ… å·¥ä½œæµè¿è¡Œè¯¦æƒ…å·²è·å–")
                print(f"       - çŠ¶æ€: {run_detail.get('status', 'N/A')}")
                print(f"       - è€—æ—¶: {run_detail.get('elapsed_time', 0):.2f} ç§’")
                print(f"       - Token æ¶ˆè€—: {run_detail.get('total_tokens', 0)}")
                print(f"       - æ€»æ­¥æ•°: {run_detail.get('total_steps', 0)}")
                if run_detail.get("error"):
                    print(f"       - é”™è¯¯: {run_detail.get('error', '')[:100]}")
                if run_detail.get("inputs"):
                    inputs_str = json.dumps(run_detail.get("inputs"), ensure_ascii=False)
                    if len(inputs_str) > 100:
                        inputs_str = inputs_str[:100] + "..."
                    print(f"       - è¾“å…¥: {inputs_str}")
                if run_detail.get("outputs"):
                    outputs_str = json.dumps(run_detail.get("outputs"), ensure_ascii=False)
                    if len(outputs_str) > 100:
                        outputs_str = outputs_str[:100] + "..."
                    print(f"       - è¾“å‡º: {outputs_str}")
            elif log.get("workflow_run_detail_error"):
                print(f"    âŒ è·å–å·¥ä½œæµè¿è¡Œè¯¦æƒ…å¤±è´¥: {log.get('workflow_run_detail_error')}")
            else:
                print(f"    âš ï¸  æœªè·å–å·¥ä½œæµè¿è¡Œè¯¦æƒ…")
            
            # èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…
            node_executions = log.get("node_executions")
            if node_executions:
                print(f"    âœ… èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…å·²è·å– ({len(node_executions)} ä¸ªèŠ‚ç‚¹)")
                for i, node in enumerate(node_executions[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ªèŠ‚ç‚¹
                    node_type = node.get("node_type", "N/A")
                    node_title = node.get("title", "N/A")
                    node_status = node.get("status", "N/A")
                    node_time = node.get("elapsed_time", 0)
                    print(f"       {i}. [{node_type}] {node_title} - {node_status} ({node_time:.2f}s)")
                if len(node_executions) > 5:
                    print(f"       ... è¿˜æœ‰ {len(node_executions) - 5} ä¸ªèŠ‚ç‚¹")
            elif log.get("node_executions_error"):
                print(f"    âŒ è·å–èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…å¤±è´¥: {log.get('node_executions_error')}")
            elif show_details and log.get("workflow_run"):
                print(f"    âš ï¸  æœªè·å–èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…ï¼ˆå¯èƒ½éœ€è¦ --with-node-executions å’Œ --console-tokenï¼‰")
            
            print()  # ç©ºè¡Œåˆ†éš”

    print(f"{'='*120}\n")


def print_summary(result: Dict[str, Any]):
    """æ‰“å°æ‘˜è¦ä¿¡æ¯"""
    total = result.get("total", 0)
    page = result.get("page", 1)
    limit = result.get("limit", 20)
    has_more = result.get("has_more", False)
    data_count = len(result.get("data", []))

    print(f"\nğŸ“Š æ—¥å¿—æ‘˜è¦:")
    print(f"   æ€»è®°å½•æ•°: {total}")
    print(f"   å½“å‰é¡µ: {page}")
    print(f"   æ¯é¡µæ•°é‡: {limit}")
    print(f"   å½“å‰é¡µè®°å½•æ•°: {data_count}")
    print(f"   æ˜¯å¦æœ‰æ›´å¤š: {'æ˜¯' if has_more else 'å¦'}")


def generate_markdown(result: Dict[str, Any], include_details: bool = False) -> str:
    """ç”Ÿæˆ Markdown æ ¼å¼çš„æŠ¥å‘Š"""
    from datetime import datetime
    
    md_lines = []
    
    # æ ‡é¢˜
    md_lines.append("# Dify å·¥ä½œæµæ‰§è¡Œæ—¥å¿—æŠ¥å‘Š")
    md_lines.append("")
    md_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    md_lines.append("")
    
    # æ•´ä½“æ‘˜è¦
    md_lines.append("## ğŸ“Š æ•´ä½“æ‘˜è¦")
    md_lines.append("")
    total = result.get("total", 0)
    page = result.get("page", 1)
    limit = result.get("limit", 20)
    has_more = result.get("has_more", False)
    data_count = len(result.get("data", []))
    
    md_lines.append("| é¡¹ç›® | å€¼ |")
    md_lines.append("|------|-----|")
    md_lines.append(f"| æ€»è®°å½•æ•° | {total} |")
    md_lines.append(f"| å½“å‰é¡µ | {page} |")
    md_lines.append(f"| æ¯é¡µæ•°é‡ | {limit} |")
    md_lines.append(f"| å½“å‰é¡µè®°å½•æ•° | {data_count} |")
    md_lines.append(f"| æ˜¯å¦æœ‰æ›´å¤š | {'æ˜¯' if has_more else 'å¦'} |")
    md_lines.append("")
    
    # ç»Ÿè®¡ä¿¡æ¯
    logs = result.get("data", [])
    if logs:
        status_count = {}
        for log in logs:
            workflow_run = log.get("workflow_run", {})
            status = workflow_run.get("status", "unknown")
            status_count[status] = status_count.get(status, 0) + 1
        
        md_lines.append("### çŠ¶æ€ç»Ÿè®¡")
        md_lines.append("")
        md_lines.append("| çŠ¶æ€ | æ•°é‡ |")
        md_lines.append("|------|------|")
        for status, count in sorted(status_count.items()):
            md_lines.append(f"| {status} | {count} |")
        md_lines.append("")
    
    # æŒ‰ ID è¯¦ç»†å±•ç¤ºæ¯æ¡æ—¥å¿—
    md_lines.append("## ğŸ“‹ æ—¥å¿—è¯¦æƒ…")
    md_lines.append("")
    
    for i, log in enumerate(logs, 1):
        log_id = log.get("id", "N/A")
        workflow_run = log.get("workflow_run", {})
        
        md_lines.append(f"### {i}. æ—¥å¿— ID: `{log_id}`")
        md_lines.append("")
        
        # åŸºæœ¬ä¿¡æ¯
        md_lines.append("#### åŸºæœ¬ä¿¡æ¯")
        md_lines.append("")
        md_lines.append("| å­—æ®µ | å€¼ |")
        md_lines.append("|------|-----|")
        md_lines.append(f"| æ—¥å¿—ID | `{log_id}` |")
        md_lines.append(f"| çŠ¶æ€ | {workflow_run.get('status', 'N/A')} |")
        md_lines.append(f"| åˆ›å»ºæ—¶é—´ | {format_timestamp(log.get('created_at'))} |")
        md_lines.append(f"| è€—æ—¶ | {workflow_run.get('elapsed_time', 0):.2f} ç§’ |")
        md_lines.append(f"| æ¥æº | {log.get('created_from', 'N/A')} |")
        
        # åˆ›å»ºè€…ä¿¡æ¯
        created_by_account = log.get("created_by_account", {})
        created_by_end_user = log.get("created_by_end_user", {})
        if created_by_account:
            creator = created_by_account.get("email", "N/A")
            creator_type = "è´¦æˆ·"
        elif created_by_end_user:
            creator = created_by_end_user.get("session_id", "N/A")
            creator_type = "ç»ˆç«¯ç”¨æˆ·"
        else:
            creator = "N/A"
            creator_type = "N/A"
        
        md_lines.append(f"| åˆ›å»ºè€…ç±»å‹ | {creator_type} |")
        md_lines.append(f"| åˆ›å»ºè€… | {creator} |")
        md_lines.append("")
        
        # å·¥ä½œæµè¿è¡Œè¯¦æƒ…
        run_detail = log.get("workflow_run_detail")
        if run_detail:
            md_lines.append("#### å·¥ä½œæµè¿è¡Œè¯¦æƒ…")
            md_lines.append("")
            md_lines.append("| å­—æ®µ | å€¼ |")
            md_lines.append("|------|-----|")
            md_lines.append(f"| è¿è¡ŒID | `{run_detail.get('id', 'N/A')}` |")
            md_lines.append(f"| çŠ¶æ€ | {run_detail.get('status', 'N/A')} |")
            md_lines.append(f"| è€—æ—¶ | {run_detail.get('elapsed_time', 0):.2f} ç§’ |")
            md_lines.append(f"| Token æ¶ˆè€— | {run_detail.get('total_tokens', 0)} |")
            md_lines.append(f"| æ€»æ­¥æ•° | {run_detail.get('total_steps', 0)} |")
            md_lines.append(f"| å¼‚å¸¸æ•°é‡ | {run_detail.get('exceptions_count', 0)} |")
            
            if run_detail.get("error"):
                md_lines.append(f"| é”™è¯¯ä¿¡æ¯ | {run_detail.get('error', '')} |")
            
            if run_detail.get("created_at"):
                md_lines.append(f"| åˆ›å»ºæ—¶é—´ | {format_timestamp(run_detail.get('created_at'))} |")
            if run_detail.get("finished_at"):
                md_lines.append(f"| å®Œæˆæ—¶é—´ | {format_timestamp(run_detail.get('finished_at'))} |")
            
            md_lines.append("")
            
            # è¾“å…¥
            if run_detail.get("inputs"):
                md_lines.append("##### è¾“å…¥å‚æ•°")
                md_lines.append("")
                md_lines.append("```json")
                md_lines.append(format_json_for_markdown(run_detail.get("inputs")))
                md_lines.append("```")
                md_lines.append("")
            
            # è¾“å‡º
            if run_detail.get("outputs"):
                md_lines.append("##### è¾“å‡ºç»“æœ")
                md_lines.append("")
                md_lines.append("```json")
                md_lines.append(format_json_for_markdown(run_detail.get("outputs")))
                md_lines.append("```")
                md_lines.append("")
        elif log.get("workflow_run_detail_error"):
            md_lines.append("#### å·¥ä½œæµè¿è¡Œè¯¦æƒ…")
            md_lines.append("")
            md_lines.append(f"âŒ è·å–å¤±è´¥: {log.get('workflow_run_detail_error')}")
            md_lines.append("")
        
        # èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…
        node_executions = log.get("node_executions")
        if node_executions:
            md_lines.append("#### èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…")
            md_lines.append("")
            md_lines.append(f"å…± {len(node_executions)} ä¸ªèŠ‚ç‚¹")
            md_lines.append("")
            
            for j, node in enumerate(node_executions, 1):
                md_lines.append(f"##### èŠ‚ç‚¹ {j}: {node.get('title', 'N/A')}")
                md_lines.append("")
                md_lines.append("| å­—æ®µ | å€¼ |")
                md_lines.append("|------|-----|")
                md_lines.append(f"| èŠ‚ç‚¹ID | `{node.get('node_id', 'N/A')}` |")
                md_lines.append(f"| èŠ‚ç‚¹ç±»å‹ | {node.get('node_type', 'N/A')} |")
                md_lines.append(f"| æ ‡é¢˜ | {node.get('title', 'N/A')} |")
                md_lines.append(f"| çŠ¶æ€ | {node.get('status', 'N/A')} |")
                md_lines.append(f"| è€—æ—¶ | {node.get('elapsed_time', 0):.2f} ç§’ |")
                md_lines.append(f"| åºå· | {node.get('index', 'N/A')} |")
                
                if node.get("predecessor_node_id"):
                    md_lines.append(f"| å‰ç½®èŠ‚ç‚¹ | `{node.get('predecessor_node_id')}` |")
                
                if node.get("error"):
                    md_lines.append(f"| é”™è¯¯ä¿¡æ¯ | {node.get('error', '')} |")
                
                if node.get("created_at"):
                    md_lines.append(f"| åˆ›å»ºæ—¶é—´ | {format_timestamp(node.get('created_at'))} |")
                if node.get("finished_at"):
                    md_lines.append(f"| å®Œæˆæ—¶é—´ | {format_timestamp(node.get('finished_at'))} |")
                
                md_lines.append("")
                
                # èŠ‚ç‚¹è¾“å…¥
                if node.get("inputs"):
                    md_lines.append("**è¾“å…¥:**")
                    md_lines.append("")
                    md_lines.append("```json")
                    md_lines.append(format_json_for_markdown(node.get("inputs")))
                    md_lines.append("```")
                    md_lines.append("")
                
                # èŠ‚ç‚¹å¤„ç†æ•°æ®
                if node.get("process_data"):
                    md_lines.append("**å¤„ç†æ•°æ®:**")
                    md_lines.append("")
                    md_lines.append("```json")
                    md_lines.append(format_json_for_markdown(node.get("process_data")))
                    md_lines.append("```")
                    md_lines.append("")
                
                # èŠ‚ç‚¹è¾“å‡º
                if node.get("outputs"):
                    md_lines.append("**è¾“å‡º:**")
                    md_lines.append("")
                    md_lines.append("```json")
                    md_lines.append(format_json_for_markdown(node.get("outputs")))
                    md_lines.append("```")
                    md_lines.append("")
        elif log.get("node_executions_error"):
            md_lines.append("#### èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…")
            md_lines.append("")
            md_lines.append(f"âŒ è·å–å¤±è´¥: {log.get('node_executions_error')}")
            md_lines.append("")
        
        # åˆ†éš”çº¿
        if i < len(logs):
            md_lines.append("---")
            md_lines.append("")
    
    return "\n".join(md_lines)


def generate_csv_reports(result: Dict[str, Any], output_dir: str):
    """
    ç”Ÿæˆ CSV æŠ¥å‘Šæ–‡ä»¶
    
    Args:
        result: æ—¥å¿—æ•°æ®ç»“æœ
        output_dir: è¾“å‡ºç›®å½•
    """
    import os
    
    logs = result.get("data", [])
    if not logs:
        print("âš ï¸  æ²¡æœ‰æ—¥å¿—æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆ CSV æŠ¥å‘Š")
        return
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    # æå–æ•°æ®
    qa_pairs = []  # ç”¨æˆ·é—®ç­”å¯¹
    user_stats = defaultdict(lambda: {
        "message_count": 0,
        "first_date": None,
        "last_date": None,
        "dates": set()
    })
    daily_stats = defaultdict(int)  # æ¯æ—¥æ¶ˆæ¯æ•°
    total_tokens = 0
    total_cost = 0.0
    session_ids = set()
    session_qa_map = defaultdict(list)  # æŒ‰ä¼šè¯IDåˆ†ç»„çš„é—®é¢˜
    
    # å¤„ç†æ¯æ¡æ—¥å¿—
    for idx, log in enumerate(logs, 1):
        workflow_run = log.get("workflow_run", {})
        run_detail = log.get("workflow_run_detail", {})
        node_executions = log.get("node_executions", [])
        
        # è·å–åŸºæœ¬ä¿¡æ¯
        created_at = log.get("created_at")
        if created_at:
            date_str = format_timestamp(created_at).split()[0]  # åªå–æ—¥æœŸéƒ¨åˆ†
            daily_stats[date_str] += 1
        
        # è·å–ç”¨æˆ·ID
        user_id = None
        created_by_account = log.get("created_by_account", {})
        created_by_end_user = log.get("created_by_end_user", {})
        if created_by_end_user:
            user_id = created_by_end_user.get("session_id")
        elif created_by_account:
            user_id = created_by_account.get("email")
        
        if not user_id:
            # å°è¯•ä»è¾“å…¥ä¸­è·å–
            if run_detail:
                inputs = run_detail.get("inputs", {})
                if isinstance(inputs, str):
                    try:
                        inputs = json.loads(inputs)
                    except (json.JSONDecodeError, TypeError):
                        inputs = {}
                elif not isinstance(inputs, dict):
                    inputs = {}
            else:
                inputs = {}
            user_id = inputs.get("sys.user_id") or inputs.get("sys", {}).get("user_id")
        
        # è·å–ä¼šè¯IDï¼ˆä½¿ç”¨ workflow_run_id ä½œä¸ºä¼šè¯æ ‡è¯†ï¼‰
        session_id = workflow_run.get("id") or log.get("id")
        if session_id:
            session_ids.add(session_id)
        
        # è·å–ç”¨æˆ·æé—®å’ŒAIå›ç­”
        user_query = ""
        ai_answer = ""
        attachments = []
        knowledge_base_name = ""
        document_names = []
        text_segments = []
        
        if run_detail:
            # å¤„ç† inputsï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸ï¼‰
            inputs = run_detail.get("inputs", {})
            if isinstance(inputs, str):
                try:
                    inputs = json.loads(inputs)
                except (json.JSONDecodeError, TypeError):
                    inputs = {}
            elif not isinstance(inputs, dict):
                inputs = {}
            
            # å¤„ç† outputsï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸ï¼‰
            outputs = run_detail.get("outputs", {})
            if isinstance(outputs, str):
                try:
                    outputs = json.loads(outputs)
                except (json.JSONDecodeError, TypeError):
                    outputs = {}
            elif not isinstance(outputs, dict):
                outputs = {}
            
            # ç”¨æˆ·æé—®
            user_query = inputs.get("query") or inputs.get("sys.query", "") or ""
            
            # AIå›ç­”
            ai_answer = outputs.get("text", "") or ""
            
            # é™„ä»¶
            files = inputs.get("sys.files", []) or inputs.get("sys", {}).get("files", [])
            if files:
                attachments = [f.get("name", "") or f.get("filename", "") for f in files if isinstance(f, dict)]
        
        # ä»èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…ä¸­è·å–çŸ¥è¯†åº“ä¿¡æ¯
        for node in node_executions:
            if node.get("node_type") == "knowledge-retrieval":
                node_outputs = node.get("outputs", {})
                if isinstance(node_outputs, str):
                    try:
                        node_outputs = json.loads(node_outputs)
                    except:
                        continue
                
                result_list = node_outputs.get("result", [])
                if result_list:
                    for item in result_list:
                        metadata = item.get("metadata", {})
                        if metadata:
                            dataset_name = metadata.get("dataset_name", "")
                            document_name = metadata.get("document_name", "")
                            content = item.get("content", "")
                            score = metadata.get("score", 0)
                            
                            if dataset_name and dataset_name not in knowledge_base_name:
                                if knowledge_base_name:
                                    knowledge_base_name += "; " + dataset_name
                                else:
                                    knowledge_base_name = dataset_name
                            
                            if document_name and document_name not in document_names:
                                document_names.append(document_name)
                            
                            if content:
                                segment_text = f"ç›¸ä¼¼åº¦:{score:.4f} {content[:200]}"
                                text_segments.append(segment_text)
        
        # ç»Ÿè®¡ç”¨æˆ·ä¿¡æ¯
        if user_id:
            user_stats[user_id]["message_count"] += 1
            if created_at:
                date_obj = datetime.fromtimestamp(created_at)
                if not user_stats[user_id]["first_date"] or date_obj < user_stats[user_id]["first_date"]:
                    user_stats[user_id]["first_date"] = date_obj
                if not user_stats[user_id]["last_date"] or date_obj > user_stats[user_id]["last_date"]:
                    user_stats[user_id]["last_date"] = date_obj
                user_stats[user_id]["dates"].add(date_obj.date())
        
        # ç»Ÿè®¡Tokenå’Œè´¹ç”¨
        if run_detail:
            total_tokens += run_detail.get("total_tokens", 0) or 0
            # ä»èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…ä¸­è·å–è´¹ç”¨
            for node in node_executions:
                if node.get("node_type") == "llm":
                    process_data = node.get("process_data", {})
                    if isinstance(process_data, str):
                        try:
                            process_data = json.loads(process_data)
                        except:
                            continue
                    if not isinstance(process_data, dict):
                        continue
                    usage = process_data.get("usage", {})
                    if usage and isinstance(usage, dict):
                        price = usage.get("total_price", 0)
                        # ç¡®ä¿ price æ˜¯æ•°å­—ç±»å‹
                        if isinstance(price, str):
                            try:
                                price = float(price)
                            except (ValueError, TypeError):
                                price = 0
                        elif not isinstance(price, (int, float)):
                            price = 0
                        total_cost += price
        
        # æ„å»ºé—®ç­”å¯¹ï¼ˆå…ˆæ”¶é›†ï¼Œåç»­æŒ‰ä¼šè¯æ’åºï¼‰
        qa_data = {
            "åºå·": idx,
            "ç”¨æˆ·id": user_id or "",
            "ä¼šè¯id": session_id or "",
            "é—®é¢˜æ’åº": 1,  # åç»­ä¼šé‡æ–°è®¡ç®—
            "ç”¨æˆ·æé—®": user_query,
            "é™„ä»¶åç§°": "; ".join(attachments) if attachments else "",
            "AIå›ç­”": ai_answer[:5000] if len(ai_answer) > 5000 else ai_answer,  # é™åˆ¶é•¿åº¦
            "çŸ¥è¯†åº“åç§°": knowledge_base_name,
            "å¼•ç”¨çš„æ–‡æ¡£åç§°": "; ".join(document_names[:5]),  # æœ€å¤š5ä¸ªæ–‡æ¡£
            "æ–‡æœ¬ç‰‡æ®µå†…å®¹1": text_segments[0] if text_segments else "",
            "æ–‡æœ¬ç‰‡æ®µå†…å®¹2": text_segments[1] if len(text_segments) > 1 else "",
            "æ–‡æœ¬ç‰‡æ®µå†…å®¹N": "; ".join(text_segments[2:10]) if len(text_segments) > 2 else "",  # æœ€å¤š10ä¸ªç‰‡æ®µ
            "åˆ›å»ºæ—¶é—´": format_timestamp(created_at) if created_at else "",
            "created_at": created_at,  # ç”¨äºæ’åº
        }
        
        if session_id:
            session_qa_map[session_id].append(qa_data)
        else:
            qa_pairs.append(qa_data)
    
    # æŒ‰ä¼šè¯IDåˆ†ç»„ï¼Œè®¡ç®—é—®é¢˜æ’åº
    for session_id, session_qas in session_qa_map.items():
        # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
        session_qas.sort(key=lambda x: x.get("created_at") or 0)
        # åˆ†é…é—®é¢˜æ’åº
        for order, qa in enumerate(session_qas, 1):
            qa["é—®é¢˜æ’åº"] = order
            # ç§»é™¤ä¸´æ—¶å­—æ®µ
            qa.pop("created_at", None)
            qa_pairs.append(qa)
    
    # æŒ‰åºå·æ’åºï¼ˆä¿æŒåŸå§‹é¡ºåºï¼‰
    qa_pairs.sort(key=lambda x: x["åºå·"])
    
    # 1. ç”Ÿæˆæ€»è§ˆ CSV
    overview_file = os.path.join(output_dir, "é—®ç­”ç±»åº”ç”¨æ•°-æ€»è§ˆ.csv")
    with open(overview_file, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["å¼€å§‹æ—¥æœŸ", "ç»“æŸæ—¥æœŸ", "å…¨éƒ¨æ¶ˆæ¯æ•°", "ç”¨æˆ·æ•°", "å…¨éƒ¨ä¼šè¯æ•°", "å¹³å‡ä¼šè¯äº’åŠ¨æ•°", "Tokenè¾“å‡ºé€Ÿåº¦", "ç”¨æˆ·æ»¡æ„åº¦", "è´¹ç”¨æ¶ˆè€—"])
        
        if logs:
            dates = [datetime.fromtimestamp(log.get("created_at")) for log in logs if log.get("created_at")]
            if dates:
                start_date = min(dates).strftime("%Y-%m-%d")
                end_date = max(dates).strftime("%Y-%m-%d")
            else:
                start_date = ""
                end_date = ""
            
            total_messages = len(logs)
            total_users = len(user_stats)
            total_sessions = len(session_ids)
            avg_interactions = total_messages / total_sessions if total_sessions > 0 else 0
            
            # Tokenè¾“å‡ºé€Ÿåº¦ï¼ˆç®€åŒ–è®¡ç®—ï¼šæ€»tokens/æ€»è€—æ—¶ï¼‰
            total_time = sum(workflow_run.get("elapsed_time", 0) for log in logs if log.get("workflow_run", {}).get("elapsed_time"))
            token_speed = total_tokens / total_time if total_time > 0 else 0
            
            writer.writerow([
                start_date,
                end_date,
                total_messages,
                total_users,
                total_sessions,
                f"{avg_interactions:.2f}",
                f"{token_speed:.2f} tokens/ç§’",
                "",  # ç”¨æˆ·æ»¡æ„åº¦ï¼ˆéœ€è¦é¢å¤–æ•°æ®ï¼‰
                f"{total_cost:.6f}",
            ])
        else:
            writer.writerow(["", "", "", "", "", "", "", "", ""])
    
    # 2. ç”Ÿæˆæ¯æ—¥æ¶ˆæ¯æ•° CSV
    daily_file = os.path.join(output_dir, "é—®ç­”ç±»åº”ç”¨æ•°-æ¯æ—¥æ¶ˆæ¯æ•°.csv")
    with open(daily_file, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["æ—¥æœŸ", "æ¶ˆæ¯æ•°é‡"])
        for date_str in sorted(daily_stats.keys()):
            writer.writerow([date_str, daily_stats[date_str]])
    
    # 3. ç”Ÿæˆç”¨æˆ·åˆ—è¡¨ CSV
    user_list_file = os.path.join(output_dir, "é—®ç­”ç±»åº”ç”¨æ•°-ç”¨æˆ·åˆ—è¡¨.csv")
    with open(user_list_file, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["ç”¨æˆ·ID", "æ¶ˆæ¯æ•°", "ä½¿ç”¨å¤©æ•°", "é¦–æ¬¡ä½¿ç”¨æ—¥æœŸ", "æœ€åä½¿ç”¨æ—¥æœŸ"])
        for user_id, stats in sorted(user_stats.items(), key=lambda x: x[1]["message_count"], reverse=True):
            use_days = len(stats["dates"])
            first_date = stats["first_date"].strftime("%Y-%m-%d") if stats["first_date"] else ""
            last_date = stats["last_date"].strftime("%Y-%m-%d") if stats["last_date"] else ""
            writer.writerow([user_id, stats["message_count"], use_days, first_date, last_date])
    
    # 4. ç”Ÿæˆç”¨æˆ·é—®ç­”å¯¹ CSV
    qa_file = os.path.join(output_dir, "é—®ç­”ç±»åº”ç”¨æ•°-ç”¨æˆ·é—®ç­”å¯¹.csv")
    with open(qa_file, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        # å†™å…¥è¡¨å¤´ï¼ˆç¬¬ä¸€è¡Œç©ºè¡Œï¼Œç¬¬äºŒè¡Œæ ‡é¢˜ï¼‰
        writer.writerow([""] * 13)
        writer.writerow([
            "åºå·", "ç”¨æˆ·id", "ä¼šè¯id", "é—®é¢˜æ’åºï¼ˆåŒä¸€ä¸ªä¼šè¯IDï¼Œæé—®å…ˆåé¡ºåºï¼‰",
            "ç”¨æˆ·æé—®", "é™„ä»¶åç§°ï¼šåç§°.åç¼€", "AIå›ç­”", "çŸ¥è¯†åº“åç§°", "å¼•ç”¨çš„æ–‡æ¡£åç§°",
            "æ–‡æœ¬ç‰‡æ®µå†…å®¹1ï¼ˆç›¸ä¼¼åº¦+æ–‡æœ¬å†…å®¹ï¼‰", "æ–‡æœ¬ç‰‡æ®µå†…å®¹2ï¼ˆç›¸ä¼¼åº¦+æ–‡æœ¬å†…å®¹ï¼‰",
            "æ–‡æœ¬ç‰‡æ®µå†…å®¹Nï¼ˆç›¸ä¼¼åº¦+æ–‡æœ¬å†…å®¹ï¼‰", "åˆ›å»ºæ—¶é—´"
        ])
        writer.writerow([""] * 13)
        
        # å†™å…¥æ•°æ®
        for qa in qa_pairs:
            writer.writerow([
                qa["åºå·"],
                qa["ç”¨æˆ·id"],
                qa["ä¼šè¯id"],
                qa["é—®é¢˜æ’åº"],
                qa["ç”¨æˆ·æé—®"],
                qa["é™„ä»¶åç§°"],
                qa["AIå›ç­”"],
                qa["çŸ¥è¯†åº“åç§°"],
                qa["å¼•ç”¨çš„æ–‡æ¡£åç§°"],
                qa["æ–‡æœ¬ç‰‡æ®µå†…å®¹1"],
                qa["æ–‡æœ¬ç‰‡æ®µå†…å®¹2"],
                qa["æ–‡æœ¬ç‰‡æ®µå†…å®¹N"],
                qa["åˆ›å»ºæ—¶é—´"],
            ])
        
        # å†™å…¥è¯´æ˜
        writer.writerow([""] * 13)
        writer.writerow(["æ³¨ï¼šæ­¤å¤„åŒºåˆ†æ˜¯å¦å¯ä¸Šä¼ é™„ä»¶ã€æ˜¯å¦å¼•ç”¨RAGçŸ¥è¯†åº“ï¼Œè‹¥æ— å†…å®¹ï¼Œä¸ºç©ºå³å¯ã€‚"] + [""] * 12)
        writer.writerow([""] * 13)
    
    print(f"âœ… CSV æŠ¥å‘Šå·²ç”Ÿæˆåˆ°ç›®å½•: {output_dir}")
    print(f"   - æ€»è§ˆ: {overview_file}")
    print(f"   - æ¯æ—¥æ¶ˆæ¯æ•°: {daily_file}")
    print(f"   - ç”¨æˆ·åˆ—è¡¨: {user_list_file}")
    print(f"   - ç”¨æˆ·é—®ç­”å¯¹: {qa_file}")


def main():
    parser = argparse.ArgumentParser(
        description="è·å– Dify å·¥ä½œæµåº”ç”¨æ‰§è¡Œæ—¥å¿—",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )

    # å¿…éœ€å‚æ•°
    parser.add_argument(
        "--api-token",
        required=True,
        help="åº”ç”¨ API Token (æ ¼å¼: app-xxx)",
    )
    parser.add_argument(
        "--base-url",
        required=True,
        help="Dify API åŸºç¡€ URL (ä¾‹å¦‚: https://api.dify.ai)",
    )

    # è¿‡æ»¤å‚æ•°
    parser.add_argument(
        "--keyword",
        help="æœç´¢å…³é”®è¯ï¼ˆåŒ¹é…è¾“å…¥ã€è¾“å‡ºã€ä¼šè¯IDæˆ–å·¥ä½œæµè¿è¡ŒIDï¼‰",
    )
    parser.add_argument(
        "--status",
        choices=["succeeded", "failed", "stopped", "partial-succeeded"],
        help="è¿‡æ»¤æ‰§è¡ŒçŠ¶æ€",
    )
    parser.add_argument(
        "--before",
        dest="created_at_before",
        help="è¿‡æ»¤åœ¨æ­¤æ—¶é—´ä¹‹å‰åˆ›å»ºçš„æ—¥å¿— (ISO 8601 æ ¼å¼, ä¾‹å¦‚: 2024-01-01T00:00:00Z)",
    )
    parser.add_argument(
        "--after",
        dest="created_at_after",
        help="è¿‡æ»¤åœ¨æ­¤æ—¶é—´ä¹‹ååˆ›å»ºçš„æ—¥å¿— (ISO 8601 æ ¼å¼, ä¾‹å¦‚: 2024-01-01T00:00:00Z)",
    )
    parser.add_argument(
        "--end-user-session-id",
        dest="created_by_end_user_session_id",
        help="æŒ‰ç»ˆç«¯ç”¨æˆ·ä¼šè¯IDè¿‡æ»¤",
    )
    parser.add_argument(
        "--account",
        dest="created_by_account",
        help="æŒ‰è´¦æˆ·é‚®ç®±è¿‡æ»¤",
    )

    # åˆ†é¡µå‚æ•°
    parser.add_argument(
        "--page",
        type=int,
        default=1,
        help="é¡µç  (é»˜è®¤: 1)",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=20,
        help="æ¯é¡µæ•°é‡ (é»˜è®¤: 20, æœ€å¤§: 100)",
    )

    # è¾“å‡ºé€‰é¡¹
    parser.add_argument(
        "--output",
        "-o",
        help="è¾“å‡ºåˆ° JSON æ–‡ä»¶",
    )
    parser.add_argument(
        "--output-md",
        help="è¾“å‡ºåˆ° Markdown æ–‡ä»¶",
    )
    parser.add_argument(
        "--output-csv-dir",
        help="è¾“å‡º CSV æŠ¥å‘Šåˆ°æŒ‡å®šç›®å½•ï¼ˆç”Ÿæˆé—®ç­”ç±»åº”ç”¨æ•°ç›¸å…³çš„ CSV æ–‡ä»¶ï¼‰",
    )
    parser.add_argument(
        "--format",
        choices=["table", "json"],
        default="table",
        help="è¾“å‡ºæ ¼å¼ (é»˜è®¤: table)",
    )
    parser.add_argument(
        "--fetch-all",
        action="store_true",
        help="è·å–æ‰€æœ‰æ—¥å¿—ï¼ˆè‡ªåŠ¨ç¿»é¡µï¼‰",
    )
    parser.add_argument(
        "--max-pages",
        type=int,
        help="æœ€å¤§é¡µæ•°é™åˆ¶ï¼ˆä»…åœ¨ä½¿ç”¨ --fetch-all æ—¶æœ‰æ•ˆï¼‰",
    )
    
    # è¯¦ç»†ä¿¡æ¯é€‰é¡¹
    parser.add_argument(
        "--with-details",
        action="store_true",
        help="è·å–æ¯æ¡æ—¥å¿—çš„è¯¦ç»†ä¿¡æ¯ï¼ˆå·¥ä½œæµè¿è¡Œè¯¦æƒ…ï¼‰",
    )
    parser.add_argument(
        "--with-node-executions",
        action="store_true",
        help="è·å–èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…ï¼ˆéœ€è¦ --console-tokenï¼‰",
    )
    parser.add_argument(
        "--console-token",
        help="Console API Tokenï¼ˆç”¨æˆ·ç™»å½•åè·å¾—çš„ JWT tokenï¼Œç”¨äºè·å–èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…ï¼Œå¯é€‰ï¼‰",
    )
    parser.add_argument(
        "--console-email",
        help="Console ç™»å½•é‚®ç®±ï¼ˆç”¨äºè‡ªåŠ¨è·å– Console Tokenï¼Œå¯é€‰ï¼‰",
    )
    parser.add_argument(
        "--console-password",
        help="Console ç™»å½•å¯†ç ï¼ˆç”¨äºè‡ªåŠ¨è·å– Console Tokenï¼Œå¯é€‰ï¼‰",
    )
    parser.add_argument(
        "--app-id",
        help="åº”ç”¨IDï¼ˆå¦‚æœæ—¥å¿—ä¸­æ²¡æœ‰app_idå­—æ®µï¼Œéœ€è¦æ‰‹åŠ¨æŒ‡å®šï¼‰",
    )

    args = parser.parse_args()

    # éªŒè¯å‚æ•°
    if args.limit < 1 or args.limit > 100:
        print("é”™è¯¯: --limit å¿…é¡»åœ¨ 1-100 ä¹‹é—´")
        sys.exit(1)

    if args.page < 1:
        print("é”™è¯¯: --page å¿…é¡»å¤§äº 0")
        sys.exit(1)

    # éªŒè¯å‚æ•°
    if args.with_node_executions:
        if not args.console_token and not (args.console_email and args.console_password):
            print("é”™è¯¯: ä½¿ç”¨ --with-node-executions æ—¶å¿…é¡»æä¾› --console-token æˆ– --console-email + --console-password")
            sys.exit(1)

    # åˆ›å»ºè·å–å™¨
    try:
        fetcher = WorkflowLogFetcher(
            args.base_url,
            args.api_token,
            console_token=args.console_token,
            console_email=args.console_email,
            console_password=args.console_password,
        )
    except Exception as e:
        print(f"é”™è¯¯: åˆå§‹åŒ–å¤±è´¥ - {e}")
        sys.exit(1)

    # è·å–æ—¥å¿—
    try:
        if args.fetch_all:
            print("æ­£åœ¨è·å–æ‰€æœ‰æ—¥å¿—...")
            logs = fetcher.fetch_all_logs(
                keyword=args.keyword,
                status=args.status,
                created_at_before=args.created_at_before,
                created_at_after=args.created_at_after,
                created_by_end_user_session_id=args.created_by_end_user_session_id,
                created_by_account=args.created_by_account,
                limit=args.limit,
                max_pages=args.max_pages,
            )
            result = {
                "total": len(logs),
                "data": logs,
                "has_more": False,
            }
        else:
            result = fetcher.fetch_logs(
                keyword=args.keyword,
                status=args.status,
                created_at_before=args.created_at_before,
                created_at_after=args.created_at_after,
                created_by_end_user_session_id=args.created_by_end_user_session_id,
                created_by_account=args.created_by_account,
                page=args.page,
                limit=args.limit,
            )
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        sys.exit(1)

    # è·å–è¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if args.with_details or args.with_node_executions:
        print("æ­£åœ¨è·å–è¯¦ç»†ä¿¡æ¯...")
        logs = result.get("data", [])
        enriched_logs = []
        
        for i, log in enumerate(logs, 1):
            print(f"  å¤„ç†æ—¥å¿— {i}/{len(logs)}...", end="\r")
            try:
                enriched_log = fetcher.enrich_log_with_details(
                    log.copy(),
                    default_app_id=args.app_id,
                    include_node_executions=args.with_node_executions
                )
                enriched_logs.append(enriched_log)
            except Exception as e:
                print(f"\n  è­¦å‘Š: è·å–æ—¥å¿— {log.get('id', 'unknown')} çš„è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
                # å³ä½¿å¤±è´¥ä¹Ÿæ·»åŠ é”™è¯¯ä¿¡æ¯
                log["enrichment_error"] = str(e)
                enriched_logs.append(log)  # ä½¿ç”¨åŸå§‹æ—¥å¿—
        
        print(f"\nâœ… å·²å¤„ç† {len(enriched_logs)} æ¡æ—¥å¿—")
        result["data"] = enriched_logs

    # è¾“å‡ºç»“æœ
    if args.output_csv_dir:
        # ç”Ÿæˆ CSV æŠ¥å‘Šï¼ˆéœ€è¦è¯¦ç»†ä¿¡æ¯ï¼‰
        if not (args.with_details or args.with_node_executions):
            print("âš ï¸  ç”Ÿæˆ CSV æŠ¥å‘Šéœ€è¦è¯¦ç»†ä¿¡æ¯ï¼Œè‡ªåŠ¨å¯ç”¨ --with-details")
            # é‡æ–°è·å–è¯¦ç»†ä¿¡æ¯
            logs = result.get("data", [])
            enriched_logs = []
            for i, log in enumerate(logs, 1):
                print(f"  å¤„ç†æ—¥å¿— {i}/{len(logs)}...", end="\r")
                try:
                    enriched_log = fetcher.enrich_log_with_details(
                        log.copy(),
                        default_app_id=args.app_id,
                        include_node_executions=args.with_node_executions
                    )
                    enriched_logs.append(enriched_log)
                except Exception as e:
                    log["enrichment_error"] = str(e)
                    enriched_logs.append(log)
            print(f"\nâœ… å·²å¤„ç† {len(enriched_logs)} æ¡æ—¥å¿—")
            result["data"] = enriched_logs
        
        generate_csv_reports(result, args.output_csv_dir)
    elif args.output_md:
        # ä¿å­˜ä¸º Markdown æ–‡ä»¶
        show_details = args.with_details or args.with_node_executions
        md_content = generate_markdown(result, include_details=show_details)
        with open(args.output_md, "w", encoding="utf-8") as f:
            f.write(md_content)
        print(f"âœ… Markdown æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output_md}")
        print(f"   æ€»è®°å½•æ•°: {len(result.get('data', []))}")
        print(f"   åŒ…å«è¯¦ç»†ä¿¡æ¯: {'æ˜¯' if show_details else 'å¦'}")
    elif args.output:
        # ä¿å­˜åˆ° JSON æ–‡ä»¶
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ—¥å¿—å·²ä¿å­˜åˆ°: {args.output}")
        print(f"   æ€»è®°å½•æ•°: {len(result.get('data', []))}")
    else:
        # æ‰“å°åˆ°æ§åˆ¶å°
        if args.format == "json":
            print(json.dumps(result, ensure_ascii=False, indent=2))
        else:
            print_summary(result)
            show_details = args.with_details or args.with_node_executions
            print_logs_table(result.get("data", []), show_details=show_details)
            
            # å¦‚æœæœ‰è¯¦ç»†ä¿¡æ¯ä½†æ²¡æœ‰æ˜¾ç¤ºï¼Œæç¤ºç”¨æˆ·ä½¿ç”¨ JSON æ ¼å¼
            if show_details:
                print("\nğŸ’¡ æç¤º: è¦æŸ¥çœ‹å®Œæ•´çš„è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬å®Œæ•´çš„è¾“å…¥/è¾“å‡ºå’Œæ‰€æœ‰èŠ‚ç‚¹ï¼‰ï¼Œè¯·ä½¿ç”¨:")
                print("   --format json æˆ– --output <filename>.json æˆ– --output-md <filename>.md")


if __name__ == "__main__":
    main()
