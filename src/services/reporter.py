"""æŠ¥å‘Šç”ŸæˆæœåŠ¡"""

import csv
import json
import os
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

from src.core.logger import get_logger
from src.utils.formatters import format_timestamp

logger = get_logger(__name__)


class ReportGenerator:
    """æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: str):
        """
        åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_csv_reports(self, result: Dict[str, Any]) -> List[str]:
        """
        ç”Ÿæˆ CSV æŠ¥å‘Šæ–‡ä»¶
        
        Args:
            result: æ—¥å¿—æ•°æ®ç»“æœ
        
        Returns:
            ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        logs = result.get("data", [])
        if not logs:
            logger.warning("æ²¡æœ‰æ—¥å¿—æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆ CSV æŠ¥å‘Š")
            return []
        
        # æå–æ•°æ®
        qa_pairs = []
        user_stats = defaultdict(lambda: {
            "message_count": 0,
            "first_date": None,
            "last_date": None,
            "dates": set()
        })
        daily_stats = defaultdict(int)
        total_tokens = 0
        total_cost = 0.0
        session_ids = set()
        session_qa_map = defaultdict(list)
        
        # å¤„ç†æ¯æ¡æ—¥å¿—
        for idx, log in enumerate(logs, 1):
            workflow_run = log.get("workflow_run", {})
            run_detail = log.get("workflow_run_detail", {})
            node_executions = log.get("node_executions", [])
            
            # è·å–åŸºæœ¬ä¿¡æ¯
            created_at = log.get("created_at")
            if created_at:
                date_str = format_timestamp(created_at).split()[0]
                daily_stats[date_str] += 1
            
            # è·å–ç”¨æˆ·ID
            user_id = None
            created_by_account = log.get("created_by_account", {})
            created_by_end_user = log.get("created_by_end_user", {})
            if created_by_end_user:
                user_id = created_by_end_user.get("session_id")
            elif created_by_account:
                user_id = created_by_account.get("email")
            
            if not user_id and run_detail:
                inputs = run_detail.get("inputs", {})
                if isinstance(inputs, str):
                    try:
                        inputs = json.loads(inputs)
                    except (json.JSONDecodeError, TypeError):
                        inputs = {}
                elif not isinstance(inputs, dict):
                    inputs = {}
                user_id = inputs.get("sys.user_id") or inputs.get("sys", {}).get("user_id")
            
            # è·å–ä¼šè¯ID
            session_id = workflow_run.get("id") or log.get("id")
            if session_id:
                session_ids.add(session_id)
            
            # è·å–ç”¨æˆ·æé—®å’ŒAIå›ç­”
            user_query = ""
            ai_answer = ""
            attachments = []
            # ä¸ä½¿ç”¨å»é‡ï¼Œç›´æ¥æŒ‰é¡ºåºå­˜å‚¨æ‰€æœ‰æŸ¥åˆ°çš„å†…å®¹ï¼ˆåŒ…æ‹¬é‡å¤ï¼‰
            # æ¯ä¸ªç‰‡æ®µéƒ½å•ç‹¬è®°å½•ï¼Œä¿æŒåŸå§‹é¡ºåº
            segments_list = []  # å­˜å‚¨æ‰€æœ‰ç‰‡æ®µï¼Œæ ¼å¼: (çŸ¥è¯†åº“åç§°, æ–‡æ¡£åç§°, ç‰‡æ®µå†…å®¹, ç›¸ä¼¼åº¦)
            
            if run_detail:
                # å¤„ç† inputs
                inputs = run_detail.get("inputs", {})
                if isinstance(inputs, str):
                    try:
                        inputs = json.loads(inputs)
                    except (json.JSONDecodeError, TypeError):
                        inputs = {}
                elif not isinstance(inputs, dict):
                    inputs = {}
                
                # å¤„ç† outputs
                outputs = run_detail.get("outputs", {})
                if isinstance(outputs, str):
                    try:
                        outputs = json.loads(outputs)
                    except (json.JSONDecodeError, TypeError):
                        outputs = {}
                elif not isinstance(outputs, dict):
                    outputs = {}
                
                user_query = inputs.get("query") or inputs.get("sys.query", "") or ""
                ai_answer = outputs.get("text", "") or ""
                
                files = inputs.get("sys.files", []) or inputs.get("sys", {}).get("files", [])
                if files:
                    attachments = [f.get("name", "") or f.get("filename", "") for f in files if isinstance(f, dict)]
            
            # ä»èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…ä¸­è·å–çŸ¥è¯†åº“ä¿¡æ¯
            for node in node_executions:
                if node.get("node_type") == "knowledge-retrieval":
                    node_outputs = node.get("outputs", {})
                    if isinstance(node_outputs, str):
                        try:
                            node_outputs = json.loads(node_outputs)
                        except:
                            continue
                    
                    result_list = node_outputs.get("result", [])
                    if result_list:
                        for item in result_list:
                            metadata = item.get("metadata", {})
                            if metadata:
                                dataset_name = metadata.get("dataset_name", "")
                                document_name = metadata.get("document_name", "")
                                content = item.get("content", "")
                                score = metadata.get("score", 0)
                                
                                # ä¸å»é‡ï¼Œç›´æ¥è®°å½•æ‰€æœ‰æŸ¥åˆ°çš„å†…å®¹ï¼ˆåŒ…æ‹¬é‡å¤ï¼‰
                                if dataset_name and document_name and content:
                                    clean_dataset = dataset_name.replace("...", "").strip()
                                    clean_document = document_name.replace("...", "").strip()
                                    
                                    # ç›¸ä¼¼åº¦å’Œæ–‡æœ¬å†…å®¹æ¢è¡Œæ˜¾ç¤º
                                    segment_text = f"ç›¸ä¼¼åº¦:{score:.4f}\n{content[:200]}"
                                    
                                    # ç›´æ¥è¿½åŠ ï¼Œä¸å»é‡
                                    segments_list.append((clean_dataset, clean_document, segment_text))
            
            # ç»Ÿè®¡ç”¨æˆ·ä¿¡æ¯
            if user_id:
                user_stats[user_id]["message_count"] += 1
                if created_at:
                    date_obj = datetime.fromtimestamp(created_at)
                    if not user_stats[user_id]["first_date"] or date_obj < user_stats[user_id]["first_date"]:
                        user_stats[user_id]["first_date"] = date_obj
                    if not user_stats[user_id]["last_date"] or date_obj > user_stats[user_id]["last_date"]:
                        user_stats[user_id]["last_date"] = date_obj
                    user_stats[user_id]["dates"].add(date_obj.date())
            
            # ç»Ÿè®¡Tokenå’Œè´¹ç”¨
            if run_detail:
                total_tokens += run_detail.get("total_tokens", 0) or 0
                for node in node_executions:
                    if node.get("node_type") == "llm":
                        process_data = node.get("process_data", {})
                        if isinstance(process_data, str):
                            try:
                                process_data = json.loads(process_data)
                            except:
                                continue
                        if not isinstance(process_data, dict):
                            continue
                        usage = process_data.get("usage", {})
                        if usage and isinstance(usage, dict):
                            price = usage.get("total_price", 0)
                            if isinstance(price, str):
                                try:
                                    price = float(price)
                                except (ValueError, TypeError):
                                    price = 0
                            elif not isinstance(price, (int, float)):
                                price = 0
                            total_cost += price
            
            # æ„å»ºé—®ç­”å¯¹ï¼šæŒ‰çŸ¥è¯†åº“å’Œæ–‡æ¡£ç»„åˆå±•å¼€ä¸ºå¤šè¡Œ
            # ä¸å»é‡ï¼Œç›´æ¥æŒ‰æŸ¥åˆ°çš„å†…å®¹æ˜¾ç¤ºï¼ˆåŒ…æ‹¬é‡å¤ï¼‰
            if not segments_list:
                # æ²¡æœ‰çŸ¥è¯†åº“çš„æƒ…å†µï¼Œç”Ÿæˆä¸€è¡Œç©ºæ•°æ®
                qa_data = {
                    "åºå·": idx,
                    "ç”¨æˆ·id": user_id or "",
                    "ä¼šè¯id": session_id or "",
                    "é—®é¢˜æ’åº": 1,
                    "ç”¨æˆ·æé—®": user_query,
                    "é™„ä»¶åç§°": "; ".join(attachments) if attachments else "",
                    "AIå›ç­”": ai_answer[:5000] if len(ai_answer) > 5000 else ai_answer,
                    "çŸ¥è¯†åº“åç§°": "",
                    "å¼•ç”¨çš„æ–‡æ¡£åç§°": "",
                    "åˆ›å»ºæ—¶é—´": format_timestamp(created_at) if created_at else "",
                    "created_at": created_at,
                }
                if session_id:
                    session_qa_map[session_id].append(qa_data)
                else:
                    qa_pairs.append(qa_data)
            else:
                # æŒ‰çŸ¥è¯†åº“å’Œæ–‡æ¡£ç»„åˆåˆ†ç»„ï¼ˆä½†ä¸å»é‡ï¼Œæ¯ä¸ªç»„åˆå¯èƒ½æœ‰å¤šä¸ªç‰‡æ®µï¼‰
                # ä½¿ç”¨å­—å…¸æŒ‰ (çŸ¥è¯†åº“, æ–‡æ¡£) åˆ†ç»„ï¼Œä½†ä¿ç•™æ‰€æœ‰ç‰‡æ®µï¼ˆåŒ…æ‹¬é‡å¤ï¼‰
                kb_doc_segments = {}  # {(çŸ¥è¯†åº“, æ–‡æ¡£): [ç‰‡æ®µåˆ—è¡¨]}
                
                for kb_name, doc_name, segment_text in segments_list:
                    kb_doc_key = (kb_name, doc_name)
                    if kb_doc_key not in kb_doc_segments:
                        kb_doc_segments[kb_doc_key] = []
                    kb_doc_segments[kb_doc_key].append(segment_text)
                
                # ä¸ºæ¯ä¸ªçŸ¥è¯†åº“-æ–‡æ¡£ç»„åˆåˆ›å»ºä¸€è¡Œ
                for (kb_name, doc_name), doc_segments in kb_doc_segments.items():
                    # åŠ¨æ€ç”Ÿæˆæ–‡æœ¬ç‰‡æ®µåˆ—ï¼ˆæ¯ä¸ªç‰‡æ®µä¸€åˆ—ï¼‰
                    qa_data = {
                        "åºå·": idx,
                        "ç”¨æˆ·id": user_id or "",
                        "ä¼šè¯id": session_id or "",
                        "é—®é¢˜æ’åº": 1,
                        "ç”¨æˆ·æé—®": user_query,
                        "é™„ä»¶åç§°": "; ".join(attachments) if attachments else "",
                        "AIå›ç­”": ai_answer[:5000] if len(ai_answer) > 5000 else ai_answer,
                        "çŸ¥è¯†åº“åç§°": kb_name,
                        "å¼•ç”¨çš„æ–‡æ¡£åç§°": doc_name,
                        "åˆ›å»ºæ—¶é—´": format_timestamp(created_at) if created_at else "",
                        "created_at": created_at,
                    }
                    
                    # åŠ¨æ€æ·»åŠ æ–‡æœ¬ç‰‡æ®µåˆ—ï¼ˆæ¯ä¸ªç‰‡æ®µä¸€åˆ—ï¼Œä¸å»é‡ï¼‰
                    for i, segment in enumerate(doc_segments, 1):
                        qa_data[f"æ–‡æœ¬ç‰‡æ®µå†…å®¹{i}"] = segment
                    
                    if session_id:
                        session_qa_map[session_id].append(qa_data)
                    else:
                        qa_pairs.append(qa_data)
        
        # æŒ‰ä¼šè¯IDåˆ†ç»„ï¼Œè®¡ç®—é—®é¢˜æ’åº
        for session_id, session_qas in session_qa_map.items():
            session_qas.sort(key=lambda x: x.get("created_at") or 0)
            for order, qa in enumerate(session_qas, 1):
                qa["é—®é¢˜æ’åº"] = order
                qa.pop("created_at", None)
                qa_pairs.append(qa)
        
        qa_pairs.sort(key=lambda x: x["åºå·"])
        
        report_files = []
        
        # 1. ç”Ÿæˆæ€»è§ˆ CSV
        overview_file = self.output_dir / "é—®ç­”ç±»åº”ç”¨æ•°-æ€»è§ˆ.csv"
        with open(overview_file, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["å¼€å§‹æ—¥æœŸ", "ç»“æŸæ—¥æœŸ", "å…¨éƒ¨æ¶ˆæ¯æ•°", "ç”¨æˆ·æ•°", "å…¨éƒ¨ä¼šè¯æ•°", "å¹³å‡ä¼šè¯äº’åŠ¨æ•°", "Tokenè¾“å‡ºé€Ÿåº¦", "ç”¨æˆ·æ»¡æ„åº¦", "è´¹ç”¨æ¶ˆè€—"])
            
            if logs:
                dates = [datetime.fromtimestamp(log.get("created_at")) for log in logs if log.get("created_at")]
                if dates:
                    start_date = min(dates).strftime("%Y-%m-%d")
                    end_date = max(dates).strftime("%Y-%m-%d")
                else:
                    start_date = ""
                    end_date = ""
                
                total_messages = len(logs)
                total_users = len(user_stats)
                total_sessions = len(session_ids)
                avg_interactions = total_messages / total_sessions if total_sessions > 0 else 0
                
                total_time = sum(log.get("workflow_run", {}).get("elapsed_time", 0) for log in logs if log.get("workflow_run", {}).get("elapsed_time"))
                token_speed = total_tokens / total_time if total_time > 0 else 0
                
                writer.writerow([
                    start_date, end_date, total_messages, total_users, total_sessions,
                    f"{avg_interactions:.2f}", f"{token_speed:.2f} tokens/ç§’", "", f"{total_cost:.6f}",
                ])
            else:
                writer.writerow([""] * 9)
        
        report_files.append(str(overview_file))
        
        # 2. ç”Ÿæˆæ¯æ—¥æ¶ˆæ¯æ•° CSV
        daily_file = self.output_dir / "é—®ç­”ç±»åº”ç”¨æ•°-æ¯æ—¥æ¶ˆæ¯æ•°.csv"
        with open(daily_file, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["æ—¥æœŸ", "æ¶ˆæ¯æ•°é‡"])
            for date_str in sorted(daily_stats.keys()):
                writer.writerow([date_str, daily_stats[date_str]])
        
        report_files.append(str(daily_file))
        
        # 3. ç”Ÿæˆç”¨æˆ·åˆ—è¡¨ CSV
        user_list_file = self.output_dir / "é—®ç­”ç±»åº”ç”¨æ•°-ç”¨æˆ·åˆ—è¡¨.csv"
        with open(user_list_file, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["ç”¨æˆ·ID", "æ¶ˆæ¯æ•°", "ä½¿ç”¨å¤©æ•°", "é¦–æ¬¡ä½¿ç”¨æ—¥æœŸ", "æœ€åä½¿ç”¨æ—¥æœŸ"])
            for user_id, stats in sorted(user_stats.items(), key=lambda x: x[1]["message_count"], reverse=True):
                use_days = len(stats["dates"])
                first_date = stats["first_date"].strftime("%Y-%m-%d") if stats["first_date"] else ""
                last_date = stats["last_date"].strftime("%Y-%m-%d") if stats["last_date"] else ""
                writer.writerow([user_id, stats["message_count"], use_days, first_date, last_date])
        
        report_files.append(str(user_list_file))
        
        # 4. ç”Ÿæˆç”¨æˆ·é—®ç­”å¯¹ CSV
        # å…ˆç»Ÿè®¡æ‰€æœ‰é—®ç­”å¯¹ä¸­ï¼Œæ¯ä¸ªçŸ¥è¯†åº“-æ–‡æ¡£ç»„åˆæœ€å¤šæœ‰å¤šå°‘ä¸ªæ–‡æœ¬ç‰‡æ®µ
        max_segments = 0
        for qa in qa_pairs:
            # ç»Ÿè®¡è¯¥è¡Œæœ‰å¤šå°‘ä¸ªæ–‡æœ¬ç‰‡æ®µåˆ—
            segment_count = 0
            for key in qa.keys():
                if key.startswith("æ–‡æœ¬ç‰‡æ®µå†…å®¹"):
                    try:
                        num = int(key.replace("æ–‡æœ¬ç‰‡æ®µå†…å®¹", ""))
                        segment_count = max(segment_count, num)
                    except ValueError:
                        pass
            max_segments = max(max_segments, segment_count)
        
        # è‡³å°‘è¦æœ‰3åˆ—ï¼ˆæ–‡æœ¬ç‰‡æ®µå†…å®¹1ã€2ã€Nï¼‰ï¼Œå¦‚æœè¶…è¿‡3ä¸ªï¼Œåˆ™åŠ¨æ€å¢åŠ åˆ—
        max_segments = max(max_segments, 3)
        
        # æ„å»ºåˆ—å
        base_columns = [
            "åºå·", "ç”¨æˆ·id", "ä¼šè¯id", "é—®é¢˜æ’åºï¼ˆåŒä¸€ä¸ªä¼šè¯IDï¼Œæé—®å…ˆåé¡ºåºï¼‰",
            "ç”¨æˆ·æé—®", "é™„ä»¶åç§°ï¼šåç§°.åç¼€", "AIå›ç­”", "çŸ¥è¯†åº“åç§°", "å¼•ç”¨çš„æ–‡æ¡£åç§°",
        ]
        # åŠ¨æ€ç”Ÿæˆæ–‡æœ¬ç‰‡æ®µåˆ—å
        segment_columns = [f"æ–‡æœ¬ç‰‡æ®µå†…å®¹{i}ï¼ˆç›¸ä¼¼åº¦+æ–‡æœ¬å†…å®¹ï¼‰" for i in range(1, max_segments + 1)]
        all_columns = base_columns + segment_columns + ["åˆ›å»ºæ—¶é—´"]
        
        qa_file = self.output_dir / "é—®ç­”ç±»åº”ç”¨æ•°-ç”¨æˆ·é—®ç­”å¯¹.csv"
        with open(qa_file, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow([""] * len(all_columns))
            writer.writerow(all_columns)
            writer.writerow([""] * len(all_columns))
            
            for qa in qa_pairs:
                row = [
                    qa.get("åºå·", ""),
                    qa.get("ç”¨æˆ·id", ""),
                    qa.get("ä¼šè¯id", ""),
                    qa.get("é—®é¢˜æ’åº", ""),
                    qa.get("ç”¨æˆ·æé—®", ""),
                    qa.get("é™„ä»¶åç§°", ""),
                    qa.get("AIå›ç­”", ""),
                    qa.get("çŸ¥è¯†åº“åç§°", ""),
                    qa.get("å¼•ç”¨çš„æ–‡æ¡£åç§°", ""),
                ]
                # åŠ¨æ€å¡«å……æ–‡æœ¬ç‰‡æ®µåˆ—
                for i in range(1, max_segments + 1):
                    row.append(qa.get(f"æ–‡æœ¬ç‰‡æ®µå†…å®¹{i}", ""))
                row.append(qa.get("åˆ›å»ºæ—¶é—´", ""))
                writer.writerow(row)
            
            writer.writerow([""] * len(all_columns))
            writer.writerow(["æ³¨ï¼šæ­¤å¤„åŒºåˆ†æ˜¯å¦å¯ä¸Šä¼ é™„ä»¶ã€æ˜¯å¦å¼•ç”¨RAGçŸ¥è¯†åº“ï¼Œè‹¥æ— å†…å®¹ï¼Œä¸ºç©ºå³å¯ã€‚"] + [""] * (len(all_columns) - 1))
            writer.writerow([""] * len(all_columns))
        
        report_files.append(str(qa_file))
        
        logger.info(f"CSV æŠ¥å‘Šå·²ç”Ÿæˆ: {len(report_files)} ä¸ªæ–‡ä»¶")
        return report_files
    
    def generate_markdown_report(self, result: Dict[str, Any], include_details: bool = False) -> str:
        """
        ç”Ÿæˆ Markdown æ ¼å¼çš„æŠ¥å‘Š
        
        Args:
            result: æ—¥å¿—æ•°æ®ç»“æœ
            include_details: æ˜¯å¦åŒ…å«è¯¦ç»†ä¿¡æ¯
        
        Returns:
            Markdown æŠ¥å‘Šå†…å®¹
        """
        from src.utils.formatters import format_json_for_markdown
        
        md_lines = []
        md_lines.append("# Dify å·¥ä½œæµæ‰§è¡Œæ—¥å¿—æŠ¥å‘Š")
        md_lines.append("")
        md_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        md_lines.append("")
        
        # æ•´ä½“æ‘˜è¦
        md_lines.append("## ğŸ“Š æ•´ä½“æ‘˜è¦")
        md_lines.append("")
        total = result.get("total", 0)
        page = result.get("page", 1)
        limit = result.get("limit", 20)
        has_more = result.get("has_more", False)
        data_count = len(result.get("data", []))
        
        md_lines.append("| é¡¹ç›® | å€¼ |")
        md_lines.append("|------|-----|")
        md_lines.append(f"| æ€»è®°å½•æ•° | {total} |")
        md_lines.append(f"| å½“å‰é¡µ | {page} |")
        md_lines.append(f"| æ¯é¡µæ•°é‡ | {limit} |")
        md_lines.append(f"| å½“å‰é¡µè®°å½•æ•° | {data_count} |")
        md_lines.append(f"| æ˜¯å¦æœ‰æ›´å¤š | {'æ˜¯' if has_more else 'å¦'} |")
        md_lines.append("")
        
        # ç»Ÿè®¡ä¿¡æ¯
        logs = result.get("data", [])
        if logs:
            status_count = {}
            for log in logs:
                workflow_run = log.get("workflow_run", {})
                status = workflow_run.get("status", "unknown")
                status_count[status] = status_count.get(status, 0) + 1
            
            md_lines.append("### çŠ¶æ€ç»Ÿè®¡")
            md_lines.append("")
            md_lines.append("| çŠ¶æ€ | æ•°é‡ |")
            md_lines.append("|------|------|")
            for status, count in sorted(status_count.items()):
                md_lines.append(f"| {status} | {count} |")
            md_lines.append("")
        
        # è¯¦ç»†æ—¥å¿—ï¼ˆå¦‚æœåŒ…å«ï¼‰
        if include_details:
            md_lines.append("## ğŸ“‹ æ—¥å¿—è¯¦æƒ…")
            md_lines.append("")
            
            for i, log in enumerate(logs, 1):
                log_id = log.get("id", "N/A")
                workflow_run = log.get("workflow_run", {})
                
                md_lines.append(f"### {i}. æ—¥å¿— ID: `{log_id}`")
                md_lines.append("")
                md_lines.append("#### åŸºæœ¬ä¿¡æ¯")
                md_lines.append("")
                md_lines.append("| å­—æ®µ | å€¼ |")
                md_lines.append("|------|-----|")
                md_lines.append(f"| æ—¥å¿—ID | `{log_id}` |")
                md_lines.append(f"| çŠ¶æ€ | {workflow_run.get('status', 'N/A')} |")
                md_lines.append(f"| åˆ›å»ºæ—¶é—´ | {format_timestamp(log.get('created_at'))} |")
                md_lines.append(f"| è€—æ—¶ | {workflow_run.get('elapsed_time', 0):.2f} ç§’ |")
                md_lines.append("")
                
                # å·¥ä½œæµè¿è¡Œè¯¦æƒ…
                run_detail = log.get("workflow_run_detail")
                if run_detail:
                    md_lines.append("#### å·¥ä½œæµè¿è¡Œè¯¦æƒ…")
                    md_lines.append("")
                    if run_detail.get("inputs"):
                        md_lines.append("##### è¾“å…¥å‚æ•°")
                        md_lines.append("")
                        md_lines.append("```json")
                        md_lines.append(format_json_for_markdown(run_detail.get("inputs")))
                        md_lines.append("```")
                        md_lines.append("")
                    
                    if run_detail.get("outputs"):
                        md_lines.append("##### è¾“å‡ºç»“æœ")
                        md_lines.append("")
                        md_lines.append("```json")
                        md_lines.append(format_json_for_markdown(run_detail.get("outputs")))
                        md_lines.append("```")
                        md_lines.append("")
                
                if i < len(logs):
                    md_lines.append("---")
                    md_lines.append("")
        
        return "\n".join(md_lines)
